#+STARTUP: overview
#+STARTUP: hideblocks

* Overview (03/21/2017)
  https://prometheus.io/docs/introduction/overview/
** Update on 03/10/2018, v2.2
   https://prometheus.io/blog/2017/11/08/announcing-prometheus-2-0/
   https://coreos.com/blog/prometheus-2.0-released
* Prometheus Server
** TargetManager (retrieval)
   TargetManager is responsible to scrape metrics from all configured targets:
   it runs a goroutine per target.
** RuleManager
   There are two kinds of rules: recording rule and alerting rule.
    - Recording rules allow you to precompute frequently needed or computationally
      expensive expressions and save their result as a new set of time series.
    - Alerting rules allow you to define alert conditions based on Prometheus
      expression language expressions and to send notifications about firing
      alerts to an external service.
    All rules (recording and alerting) are evaluated at a regular interval. If
    the evaluated rule is alerting rule and is not pending, then rule manager
    will try to send the rules. RuleManager runs a goroutine per rule group (a
    group of related rules).
** Notifier
   Notifier is responsible to send alerts to AlertManager. Notifier runs in a
   goroutine and exposes a 'Send' method to its caller (called via RuleManager
   whenever an alert is triggered via a rule).
** PromQL
   PromQL is a package (not a running gorouting) that handles query, parsing
   of prometheus rules. The main datastructure is 'Engine' and an interface
   Queryable. Frontend, or generally, prometheus API, uses the interface
   methods to query time series. Local storage implements the interface, while
   remote storage doesn't; this means to query metrics in remote storage such
   as influxdb, you need to contact influxdb directly.
** Storage (Remote)
   For remote storage, there is a StorageQueueManager which manages a queue of
   samples to be sent to the remote storage (openTSDB, influxdb, graphite). There
   is a goroutine per shard (sharding is based on metric's fingerprint) that
   queues the samples, and flush to underline storage when samples are full or
   flush interval is reached.
** Storage (Local)
   Prometheus has a sophisticated local storage subsystem. For indexes, it uses
   LevelDB. For the bulk sample data, it has its own custom storage layer, which
   organizes sample data in chunks of constant size (1024 bytes payload). These
   chunks are then stored on disk in one file per time series.
** Service Discovery
   Prometheus supports different service discovery mechanism, e.g. gce, azure,
   kubernetes, etc. If we define a service discovery section in prometheus config
   file, prometheus server (target manager) will run goroutines to utilize
   service discovery features. For example, if we define "kubernetes_sd_configs"
   in our config file, prometheus will create goroutines to list & watch kubernetes
   api server based on configured roles (node, pods, etc). For different roles,
   prometheus will automatically apply meta labels: these meta labels are well
   documented in prometheus website. For example, pod listed from kubernetes
   will have a bunch of meta labels, one of them is '__meta_kubernetes_pod_ip',
   which is attached to the pod target using 'pod.Status.PodIP', see file
   'discovery/kubernetes/pod.go'. The meta labels are provided to users to
   relabel them for their own use. These labels will be discarded during scape.
     #+BEGIN_SRC yaml
     # Example scrape config for probing services via the Blackbox Exporter.
     #
     # The relabeling allows the actual service scrape endpoint to be configured
     # via the following annotations:
     #
     # * `prometheus.io/probe`: Only probe services that have a value of `true`
     - job_name: 'kubernetes-services'

       metrics_path: /probe
       params:
         module: [http_2xx]

       kubernetes_sd_configs:
       - api_servers:
         - 'https://kubernetes.default.svc'
         in_cluster: true
         role: service

       relabel_configs:
       - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
         action: keep
         regex: true
       - source_labels: [__address__]
         target_label: __param_target
       - target_label: __address__
         replacement: blackbox
       - source_labels: [__param_target]
         target_label: instance
       - action: labelmap
         regex: __meta_kubernetes_service_label_(.+)
       - source_labels: [__meta_kubernetes_service_namespace]
         target_label: kubernetes_namespace
       - source_labels: [__meta_kubernetes_service_name]
         target_label: kubernetes_name
     #+END_SRC
   Relabel is quite important in service discovery. For the example config above,
   we have label '__meta_kubernetes_service_annotation_prometheus_io_probe' (all
   annotations will be added to meta labels), the action for this label is 'keep'
   and regex is 'true', which means only targets (service) with this label (annotation),
   and whose value is 'true' will be kept; all other targets will be dropped.
   For the label '__meta_kubernetes_service_namespace', the action is empty so
   prometheus will use default action 'replace'; target label is 'kubernetes_namespace'
   so this relabel config adds a label 'kubernetes_namespace' to all services
   and its value equals that of '__meta_kubernetes_service_namespace'.
* PushGateway
  PushGateway is simple; it exposes API endpoints to external services to push
  metrics. The metrics are buffered in memory and can optionally be flushed to
  disk.
* Exporter
  There are a number of libraries and servers which help in exporting existing
  metrics from third-party systems as Prometheus metrics. E.g. node exporter
  collects machine informations and exposes a 'host:port/metrics' endpoint for
  prometheus to scrape metrics.
* References
  https://prometheus.io/docs/introduction/overview/
  https://prometheus.io/docs/operating/configuration/
  https://movio.co/blog/prometheus-service-discovery-kubernetes/
